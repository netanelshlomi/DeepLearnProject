{"cells":[{"cell_type":"markdown","metadata":{"id":"EBbxuQ7R_O2I"},"source":["# **Deep Learning Spring 2024 - Youtube Title Classifier Using Finetuned BERT Model**\n","\n","This Project is Part of the Technion's Deep Learning Course.\n","It is one model of two model fine-tune and comperision.\n","\n","**Objective**\n","\n","To classify the YouTube trending videos into 2 categories using the BERT model, a pre-trained model fine-tuned on the Google BERT model.\n","\n","**Preparation Steps**\n","- Download the Google BERT model and tokenizer from HuggingFace.\n","- Download the dataset from Kaggle and preprocess it.\n","- Prepare the data for training and testing.\n","- Fine-tune the BERT model on the dataset.\n","- Evaluate the trained model on the test set.\n","\n","**Hardware**\n","\n","the finetune process was performed using\n","- NVIDIA GeForce RTX 3080 \n","\n","**Source Material and Code Base**\n","\n","- Google BERT model:     https://huggingface.co/google-bert/bert-base-uncased\n","- Data Set From Kaggle:  Youtube. (2024). YouTube Trending Video Dataset (updated daily) [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/8125862"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26432,"status":"ok","timestamp":1723797657340,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"M4l9dGlXl86B","outputId":"5f088d9e-3001-43ae-dd15-682283dd0644"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"OO4MHXphmcCv"},"source":["## Importing Packeges\n","and Checking if GPU is available"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73827,"status":"ok","timestamp":1723801975789,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"xVpnmbdcmPX0","outputId":"8c16a2b8-fc8e-4a71-f88b-155b50dd58e0"},"outputs":[],"source":["# !pip install peft"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6748,"status":"ok","timestamp":1723797664067,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"26sw3SDwphnB"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1723797664069,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"yNELBVOkoi-w","outputId":"b5573c35-b21e-443b-d90d-27847f991242"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: NVIDIA GeForce RTX 3080\n"]}],"source":["\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"markdown","metadata":{"id":"NzOm33pSmfr3"},"source":["## Preparing The Data\n","\n","- the original data contains several features per video\n","- in this project we focus on the $title$ as a textual feature to predict the label $CategoryID$"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"executionInfo":{"elapsed":9439,"status":"ok","timestamp":1723799216541,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"3fOZrLYgmkhr","outputId":"cdce5587-9143-48b6-bd38-7c11eff8500b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","      <th>title</th>\n","      <th>publishedAt</th>\n","      <th>channelId</th>\n","      <th>channelTitle</th>\n","      <th>categoryId</th>\n","      <th>trending_date</th>\n","      <th>tags</th>\n","      <th>view_count</th>\n","      <th>likes</th>\n","      <th>dislikes</th>\n","      <th>comment_count</th>\n","      <th>thumbnail_link</th>\n","      <th>comments_disabled</th>\n","      <th>ratings_disabled</th>\n","      <th>description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3C66w5Z0ixs</td>\n","      <td>I ASKED HER TO BE MY GIRLFRIEND...</td>\n","      <td>2020-08-11T19:20:14Z</td>\n","      <td>UCvtRTOMP2TqYqu51xNrqAzg</td>\n","      <td>Brawadis</td>\n","      <td>22</td>\n","      <td>2020-08-12T00:00:00Z</td>\n","      <td>brawadis|prank|basketball|skits|ghost|funny vi...</td>\n","      <td>1514614</td>\n","      <td>156908</td>\n","      <td>5855</td>\n","      <td>35313</td>\n","      <td>https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>M9Pmf9AB4Mo</td>\n","      <td>Apex Legends | Stories from the Outlands – “Th...</td>\n","      <td>2020-08-11T17:00:10Z</td>\n","      <td>UC0ZV6M2THA81QT9hrVWJG3A</td>\n","      <td>Apex Legends</td>\n","      <td>20</td>\n","      <td>2020-08-12T00:00:00Z</td>\n","      <td>Apex Legends|Apex Legends characters|new Apex ...</td>\n","      <td>2381688</td>\n","      <td>146739</td>\n","      <td>2794</td>\n","      <td>16549</td>\n","      <td>https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>While running her own modding shop, Ramya Pare...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>J78aPJ3VyNs</td>\n","      <td>I left youtube for a month and THIS is what ha...</td>\n","      <td>2020-08-11T16:34:06Z</td>\n","      <td>UCYzPXprvl5Y-Sf0g4vX-m6g</td>\n","      <td>jacksepticeye</td>\n","      <td>24</td>\n","      <td>2020-08-12T00:00:00Z</td>\n","      <td>jacksepticeye|funny|funny meme|memes|jacksepti...</td>\n","      <td>2038853</td>\n","      <td>353787</td>\n","      <td>2628</td>\n","      <td>40221</td>\n","      <td>https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>I left youtube for a month and this is what ha...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>kXLn3HkpjaA</td>\n","      <td>XXL 2020 Freshman Class Revealed - Official An...</td>\n","      <td>2020-08-11T16:38:55Z</td>\n","      <td>UCbg_UMjlHJg_19SZckaKajg</td>\n","      <td>XXL</td>\n","      <td>10</td>\n","      <td>2020-08-12T00:00:00Z</td>\n","      <td>xxl freshman|xxl freshmen|2020 xxl freshman|20...</td>\n","      <td>496771</td>\n","      <td>23251</td>\n","      <td>1856</td>\n","      <td>7647</td>\n","      <td>https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Subscribe to XXL → http://bit.ly/subscribe-xxl...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>VIUo6yapDbc</td>\n","      <td>Ultimate DIY Home Movie Theater for The LaBran...</td>\n","      <td>2020-08-11T15:10:05Z</td>\n","      <td>UCDVPcEbVLQgLZX0Rt6jo34A</td>\n","      <td>Mr. Kate</td>\n","      <td>26</td>\n","      <td>2020-08-12T00:00:00Z</td>\n","      <td>The LaBrant Family|DIY|Interior Design|Makeove...</td>\n","      <td>1123889</td>\n","      <td>45802</td>\n","      <td>964</td>\n","      <td>2196</td>\n","      <td>https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Transforming The LaBrant Family's empty white ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      video_id                                              title  \\\n","0  3C66w5Z0ixs                 I ASKED HER TO BE MY GIRLFRIEND...   \n","1  M9Pmf9AB4Mo  Apex Legends | Stories from the Outlands – “Th...   \n","2  J78aPJ3VyNs  I left youtube for a month and THIS is what ha...   \n","3  kXLn3HkpjaA  XXL 2020 Freshman Class Revealed - Official An...   \n","4  VIUo6yapDbc  Ultimate DIY Home Movie Theater for The LaBran...   \n","\n","            publishedAt                 channelId   channelTitle  categoryId  \\\n","0  2020-08-11T19:20:14Z  UCvtRTOMP2TqYqu51xNrqAzg       Brawadis          22   \n","1  2020-08-11T17:00:10Z  UC0ZV6M2THA81QT9hrVWJG3A   Apex Legends          20   \n","2  2020-08-11T16:34:06Z  UCYzPXprvl5Y-Sf0g4vX-m6g  jacksepticeye          24   \n","3  2020-08-11T16:38:55Z  UCbg_UMjlHJg_19SZckaKajg            XXL          10   \n","4  2020-08-11T15:10:05Z  UCDVPcEbVLQgLZX0Rt6jo34A       Mr. Kate          26   \n","\n","          trending_date                                               tags  \\\n","0  2020-08-12T00:00:00Z  brawadis|prank|basketball|skits|ghost|funny vi...   \n","1  2020-08-12T00:00:00Z  Apex Legends|Apex Legends characters|new Apex ...   \n","2  2020-08-12T00:00:00Z  jacksepticeye|funny|funny meme|memes|jacksepti...   \n","3  2020-08-12T00:00:00Z  xxl freshman|xxl freshmen|2020 xxl freshman|20...   \n","4  2020-08-12T00:00:00Z  The LaBrant Family|DIY|Interior Design|Makeove...   \n","\n","   view_count   likes  dislikes  comment_count  \\\n","0     1514614  156908      5855          35313   \n","1     2381688  146739      2794          16549   \n","2     2038853  353787      2628          40221   \n","3      496771   23251      1856           7647   \n","4     1123889   45802       964           2196   \n","\n","                                   thumbnail_link  comments_disabled  \\\n","0  https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg              False   \n","1  https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg              False   \n","2  https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg              False   \n","3  https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg              False   \n","4  https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg              False   \n","\n","   ratings_disabled                                        description  \n","0             False  SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...  \n","1             False  While running her own modding shop, Ramya Pare...  \n","2             False  I left youtube for a month and this is what ha...  \n","3             False  Subscribe to XXL → http://bit.ly/subscribe-xxl...  \n","4             False  Transforming The LaBrant Family's empty white ...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"./Dataset/US_youtube_trending_data.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1723799216543,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"PHBZWKPJqElY"},"outputs":[],"source":["# Parsing the YouTube API CategoryID to a String\n","all_category_to_string = {\n","    1: \"Film & Animation\",\n","    2: \"Autos & Vehicles\",\n","    10: \"Music\",\n","    15: \"Pets & Animals\",\n","    17: \"Sports\",\n","    18: \"Short Movies\",\n","    19: \"Travel & Events\",\n","    20: \"Gaming\",\n","    21: \"Videoblogging\",\n","    22: \"People & Blogs\",\n","    23: \"Comedy\",\n","    24: \"Entertainment\",\n","    25: \"News & Politics\",\n","    26: \"Howto & Style\",\n","    27: \"Education\",\n","    28: \"Science & Technology\",\n","    29: \"Nonprofits & Activism\",\n","    30: \"Movies\",\n","    31: \"Anime/Animation\",\n","    32: \"Action/Adventure\",\n","    33: \"Classics\",\n","    34: \"Comedy\",\n","    35: \"Documentary\",\n","    36: \"Drama\",\n","    37: \"Family\",\n","    38: \"Foreign\",\n","    39: \"Horror\",\n","    40: \"Sci-Fi/Fantasy\",\n","    41: \"Thriller\",\n","    42: \"Shorts\",\n","    43: \"Shows\",\n","    44: \"Trailers\",\n","}\n","# The only categories that we Classify\n","train_categories = {20: \"Gaming\" ,24: \"Entertainment\"}\n"]},{"cell_type":"markdown","metadata":{"id":"2jDzks_irHAR"},"source":["### Checking the Data"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1723799216544,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"Dv0zxYBlrQnT","outputId":"1152df1a-9c37-42ca-a5f3-6d90e537ef36"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>categoryId</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I ASKED HER TO BE MY GIRLFRIEND...</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Apex Legends | Stories from the Outlands – “Th...</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I left youtube for a month and THIS is what ha...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>XXL 2020 Freshman Class Revealed - Official An...</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ultimate DIY Home Movie Theater for The LaBran...</td>\n","      <td>26</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  categoryId\n","0                 I ASKED HER TO BE MY GIRLFRIEND...          22\n","1  Apex Legends | Stories from the Outlands – “Th...          20\n","2  I left youtube for a month and THIS is what ha...          24\n","3  XXL 2020 Freshman Class Revealed - Official An...          10\n","4  Ultimate DIY Home Movie Theater for The LaBran...          26"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Taking only therelevant features: Title and label: CategoryID\n","data = df[[\"title\", \"categoryId\"]]\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the Data from pre-splited files  "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                               title  tags  \\\n","0                      SMILE (2022) Ending Explained     1   \n","1               i Got a Pet Monkey For The AMP House     1   \n","2                            Rick and Morty is Real.     1   \n","3  Ashnikko - You Make Me Sick! (Official Music V...     0   \n","4  Skyrim, but if my Heart Rate goes up it spawns...     1   \n","\n","                                            sentence  \n","0  \"SMILE (2022) Ending Explained\" is a title of ...  \n","1  \"i Got a Pet Monkey For The AMP House\" is a ti...  \n","2  \"Rick and Morty is Real.\" is a title of a vide...  \n","3  \"Ashnikko - You Make Me Sick! (Official Music ...  \n","4  \"Skyrim, but if my Heart Rate goes up it spawn...  \n","                                               title  tags  \\\n","0                      SMILE (2022) Ending Explained     1   \n","1               i Got a Pet Monkey For The AMP House     1   \n","2                            Rick and Morty is Real.     1   \n","3  Ashnikko - You Make Me Sick! (Official Music V...     0   \n","4  Skyrim, but if my Heart Rate goes up it spawns...     1   \n","\n","                                            sentence  \n","0  \"SMILE (2022) Ending Explained\" is a title of ...  \n","1  \"i Got a Pet Monkey For The AMP House\" is a ti...  \n","2  \"Rick and Morty is Real.\" is a title of a vide...  \n","3  \"Ashnikko - You Make Me Sick! (Official Music ...  \n","4  \"Skyrim, but if my Heart Rate goes up it spawn...  \n","                                               title  tags  \\\n","0                      SMILE (2022) Ending Explained     1   \n","1               i Got a Pet Monkey For The AMP House     1   \n","2                            Rick and Morty is Real.     1   \n","3  Ashnikko - You Make Me Sick! (Official Music V...     0   \n","4  Skyrim, but if my Heart Rate goes up it spawns...     1   \n","\n","                                            sentence  \n","0  \"SMILE (2022) Ending Explained\" is a title of ...  \n","1  \"i Got a Pet Monkey For The AMP House\" is a ti...  \n","2  \"Rick and Morty is Real.\" is a title of a vide...  \n","3  \"Ashnikko - You Make Me Sick! (Official Music ...  \n","4  \"Skyrim, but if my Heart Rate goes up it spawn...  \n"]}],"source":["# training data\n","df = pd.read_csv(\"./Dataset/train_Dataset_v3.csv\")\n","print(df.head())\n","X_train = df[\"title\"]\n","X_train.head()\n","y_train = df[\"tags\"]\n","\n","#  val_data\n","df_val = pd.read_csv(\"./Dataset/test_Dataset_v3.csv\")\n","print(df.head())\n","X_val = df_val[\"title\"]\n","y_val = df_val[\"tags\"]\n","\n","# test data\n","df_test = pd.read_csv(\"./Dataset/test_Dataset_v3.csv\")\n","print(df.head())\n","X_test = df_test[\"title\"]\n","y_test = df_test[\"tags\"]\n"]},{"cell_type":"markdown","metadata":{"id":"2ZHsg5W9vFNY"},"source":["## BERT"]},{"cell_type":"markdown","metadata":{"id":"V-vyPIruZp_R"},"source":["the class of the model with all its functions"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# run this cell\n","import torch\n","import os\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from peft import LoraConfig, get_peft_model\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","\n","class BertClassifier:\n","    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=128, batch_size=32, num_epochs=3, learning_rate=2e-5, r=8, alpha=16):\n","        self.model_name = model_name\n","        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n","        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n","\n","        # Configuring the LoRA layer\n","        lora_config = LoraConfig(\n","            r=r,\n","            lora_alpha=alpha,\n","            lora_dropout=0.1\n","        )\n","        self.model = get_peft_model(self.model, lora_config)\n","\n","        self.max_length = max_length\n","        self.batch_size = batch_size\n","        self.num_epochs = num_epochs\n","        self.learning_rate = learning_rate\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model.to(self.device)\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","\n","    def encode_data(self, texts, labels):\n","        encoded_data = self.tokenizer.batch_encode_plus(\n","            texts,\n","            add_special_tokens=True,\n","            return_attention_mask=True,\n","            padding=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        input_ids = encoded_data['input_ids']\n","        attention_masks = encoded_data['attention_mask']\n","        labels = torch.tensor(labels.to_numpy())\n","        return TensorDataset(input_ids, attention_masks, labels)\n","\n","    def create_dataloader(self, dataset, sampler_type='random'):\n","        sampler = RandomSampler(dataset) if sampler_type == 'random' else SequentialSampler(dataset)\n","        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n","\n","    def train(self, train_texts, train_labels, val_texts, val_labels, encoded=False, encoded_train_data=None, encoded_val_data=None):\n","        if encoded and encoded_train_data is None:\n","            print(\"No encoded training data provided. Encoding the data...\")\n","            encoded = False\n","        if encoded and encoded_val_data is None:\n","            print(\"No encoded validation data provided. Encoding the data...\")\n","            encoded = False\n","\n","        train_dataset = self.encode_data(train_texts, train_labels) if not encoded else encoded_train_data\n","        val_dataset = self.encode_data(val_texts, val_labels) if not encoded else encoded_val_data\n","\n","        train_dataloader = self.create_dataloader(train_dataset)\n","        val_dataloader = self.create_dataloader(val_dataset, sampler_type='sequential')\n","\n","        best_val_loss = float('inf')\n","        for epoch in range(self.num_epochs):\n","            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n","            self.model.train()\n","            train_loss = 0\n","\n","            epoch_iterator = tqdm(train_dataloader, desc=f\"Training (Epoch {epoch+1})\", leave=False)\n","            for batch in epoch_iterator:\n","                batch = tuple(t.to(self.device) for t in batch)\n","                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","                outputs = self.model(**inputs)\n","                loss = outputs.loss\n","\n","                loss.backward()\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","\n","                train_loss += loss.item()\n","                epoch_iterator.set_postfix(loss=loss.item())\n","\n","            avg_train_loss = train_loss / len(train_dataloader)\n","            val_loss, val_metrics = self.evaluate(val_dataloader)\n","            print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n","            print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n","\n","            # Save the model if the validation loss has decreased\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                self.save_checkpoint(epoch)\n","                print(\"Validation loss improved. Model checkpoint saved.\")\n","\n","    #used for testing the model\n","    def evaluate(self, dataloader):\n","        self.model.eval()\n","        total_loss = 0\n","        all_preds = []\n","        all_labels = []\n","\n","        for batch in dataloader:\n","            batch = tuple(t.to(self.device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","                outputs = self.model(**inputs)\n","                loss = outputs.loss\n","                total_loss += loss.item()\n","\n","                logits = outputs.logits\n","                preds = torch.argmax(logits, dim=1).cpu().numpy()\n","                labels = batch[2].cpu().numpy()\n","\n","                all_preds.extend(preds)\n","                all_labels.extend(labels)\n","\n","        avg_loss = total_loss / len(dataloader)\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        report = classification_report(all_labels, all_preds, target_names=[\"Class 0\", \"Class 1\"], digits=4)\n","        conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","        print(\"\\nClassification Report:\\n\", report)\n","        print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n","\n","        metrics = {\n","            'accuracy': accuracy,\n","            'confusion_matrix': conf_matrix,\n","            'classification_report': report\n","        }\n","\n","        return avg_loss, metrics\n","\n","    # given a text of the youtube title, predict the class\n","    def predict(self, text):\n","        self.model.eval()\n","        encoded_data = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n","        input_ids = encoded_data['input_ids'].to(self.device)\n","        attention_masks = encoded_data['attention_mask'].to(self.device)\n","        with torch.no_grad():\n","            outputs = self.model(input_ids, attention_mask=attention_masks)\n","        logits = outputs.logits\n","        predicted_class = torch.argmax(logits, dim=1).item()\n","        return predicted_class\n","\n","    def save_checkpoint(self, epoch):\n","        checkpoint_path = f'checkpoint_v3_epoch_{epoch + 1}.pt'\n","        torch.save({\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'hyperparameters': {\n","                'model_name': self.model_name,\n","                'num_classes': self.model.num_labels,\n","                'max_length': self.max_length,\n","                'batch_size': self.batch_size,\n","                'num_epochs': self.num_epochs,\n","                'learning_rate': self.learning_rate\n","            }\n","        }, checkpoint_path)\n","        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n","\n","# Example usage:\n","# bert_classifier = BertClassifier(num_classes=NUM_OF_CLASSES, num_epochs=5)\n","# bert_classifier.train(train_texts, train_labels, val_texts, val_labels)\n","# eval_loss, eval_metrics = bert_classifier.evaluate(eval_dataloader)\n","# print(f'Evaluation Loss: {eval_loss}')\n"]},{"cell_type":"markdown","metadata":{"id":"VORdt2MZcN-Q"},"source":["## Using the Class to Encode our Data"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80502,"status":"ok","timestamp":1723802889762,"user":{"displayName":"Roe M","userId":"13376815491624582691"},"user_tz":-180},"id":"2JMf76UDcSgg","outputId":"9e735a72-3dd1-43b5-b588-f089edab84ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["(tensor([[  101,  2868,  1006, 16798,  2475,  1007,  4566,  4541,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  1045,  2288,  1037,  9004, 10608,  2005,  1996, 23713,  2160,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  6174,  1998, 22294,  2100,  2003,  2613,  1012,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  6683,  8238,  3683,  1011,  2017,  2191,  2033,  5305,   999,\n","          1006,  2880,  2189,  2678,  1007,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  3712, 20026,  1010,  2021,  2065,  2026,  2540,  3446,  3632,\n","          2039,  2009, 25645,  2015,  2184,  8626,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([1, 1, 1, 0, 1]))\n"]}],"source":["# checking the encoding of the model works properly \n","model = BertClassifier(num_classes=2, num_epochs=10)\n","\n","train_dataset = model.encode_data(X_train, y_train)\n","train_dataloader = model.create_dataloader(train_dataset)\n","\n","# Save the train_dataset to a file\n","# torch.save(train_dataset, \"./encoded_dataset/encoded_YT.pt\")\n","# To load the dataset later, you can use:\n","# loaded_train_dataset = torch.load('train_dataset.pt')\n","\n","print(train_dataset[:5])"]},{"cell_type":"markdown","metadata":{},"source":["### Training parameters"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trainable parameters: (109778690, 294912, 0.2686423020715587)\n"]}],"source":["def count_trainable_parameters(model):\n","    all_param = sum(p.numel() for p in model.parameters())\n","    trainable_param =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    ratio = float(trainable_param  / all_param) *100\n","    return all_param, trainable_param, ratio\n","\n"," \n","# Print the number of trainable parameters\n","print(f\"Number of trainable parameters: {count_trainable_parameters(model.model)}\")\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PeftModel(\n","  (base_model): LoraModel(\n","    (model): BertForSequenceClassification(\n","      (bert): BertModel(\n","        (embeddings): BertEmbeddings(\n","          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","          (position_embeddings): Embedding(512, 768)\n","          (token_type_embeddings): Embedding(2, 768)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (encoder): BertEncoder(\n","          (layer): ModuleList(\n","            (0-11): 12 x BertLayer(\n","              (attention): BertAttention(\n","                (self): BertSdpaSelfAttention(\n","                  (query): lora.Linear(\n","                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): lora.Linear(\n","                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (output): BertSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (intermediate): BertIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): BertOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (pooler): BertPooler(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (activation): Tanh()\n","        )\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (classifier): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["print(model.model)"]},{"cell_type":"markdown","metadata":{"id":"Ndr4n6L9aDSo"},"source":["## training the Bert Model\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"GvHHs7zVgtvQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                 \r"]},{"name":"stdout","output_type":"stream","text":["\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","     Class 0     0.7423    0.8730    0.8024      5890\n","     Class 1     0.7298    0.5309    0.6146      3805\n","\n","    accuracy                         0.7387      9695\n","   macro avg     0.7360    0.7019    0.7085      9695\n","weighted avg     0.7374    0.7387    0.7287      9695\n","\n","\n","Confusion Matrix:\n"," [[5142  748]\n"," [1785 2020]]\n","Training Loss: 0.6192, Validation Loss: 0.5208\n","Validation Accuracy: 0.7387\n","Checkpoint saved at epoch 1\n","Validation loss improved. Model checkpoint saved.\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                 \r"]},{"name":"stdout","output_type":"stream","text":["\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","     Class 0     0.7624    0.8722    0.8136      5890\n","     Class 1     0.7454    0.5792    0.6519      3805\n","\n","    accuracy                         0.7572      9695\n","   macro avg     0.7539    0.7257    0.7327      9695\n","weighted avg     0.7557    0.7572    0.7501      9695\n","\n","\n","Confusion Matrix:\n"," [[5137  753]\n"," [1601 2204]]\n","Training Loss: 0.5087, Validation Loss: 0.4841\n","Validation Accuracy: 0.7572\n","Checkpoint saved at epoch 2\n","Validation loss improved. Model checkpoint saved.\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                 \r"]},{"name":"stdout","output_type":"stream","text":["\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","     Class 0     0.7791    0.8618    0.8184      5890\n","     Class 1     0.7440    0.6218    0.6775      3805\n","\n","    accuracy                         0.7676      9695\n","   macro avg     0.7616    0.7418    0.7479      9695\n","weighted avg     0.7653    0.7676    0.7631      9695\n","\n","\n","Confusion Matrix:\n"," [[5076  814]\n"," [1439 2366]]\n","Training Loss: 0.4851, Validation Loss: 0.4666\n","Validation Accuracy: 0.7676\n","Checkpoint saved at epoch 3\n","Validation loss improved. Model checkpoint saved.\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                 \r"]}],"source":["# this function might take about an hour to run, varying in hardware \n","model.train(X_train, y_train, X_val, y_val, encoded = False)\n","print (\"finished training, now evaluating\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evalutaion of the model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","     Class 0     0.8072    0.8543    0.8301      5890\n","     Class 1     0.7521    0.6841    0.7165      3805\n","\n","    accuracy                         0.7875      9695\n","   macro avg     0.7796    0.7692    0.7733      9695\n","weighted avg     0.7856    0.7875    0.7855      9695\n","\n","\n","Confusion Matrix:\n"," [[5032  858]\n"," [1202 2603]]\n","Evaluation Loss: (0.4348954964588971, {'accuracy': 0.7875193398659103, 'confusion_matrix': array([[5032,  858],\n","       [1202, 2603]]), 'classification_report': '              precision    recall  f1-score   support\\n\\n     Class 0     0.8072    0.8543    0.8301      5890\\n     Class 1     0.7521    0.6841    0.7165      3805\\n\\n    accuracy                         0.7875      9695\\n   macro avg     0.7796    0.7692    0.7733      9695\\nweighted avg     0.7856    0.7875    0.7855      9695\\n'})\n"]}],"source":["\n","# model = load_model(\"checkpoint_best_BERT_epoch_9.pt\") # uncomment to use the the best pretrained BERT model \n","test_dataset = model.encode_data(X_test, y_test)\n","test_dataloader = model.create_dataloader(test_dataset)\n","\n","\n","eval_loss = model.evaluate(test_dataloader)\n","print(f'Evaluation Loss: {eval_loss}')"]},{"cell_type":"markdown","metadata":{"id":"SAh4kiHVgnJR"},"source":["## Saving the model"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["retrived from old was saved successfully\n"]}],"source":["import os\n","save_dir = \"./checkpoints\"\n","\n","checkpoint_path = os.path.join(save_dir, 'checkpoint_v3_new_best_model.pt')\n","torch.save({\n","    'model_state_dict': model.model.state_dict(),\n","    'optimizer_state_dict': model.optimizer.state_dict(),\n","    'hyperparameters': {\n","        'model_name': 'bert-base-uncased',\n","        'num_classes': 2,\n","        'max_length': model.max_length,\n","        'batch_size': model.batch_size,\n","        'num_epochs': model.num_epochs,\n","        'learning_rate': model.learning_rate\n","    }\n","    }, checkpoint_path)\n","print(f\"retrived from old was saved successfully\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# test an example title"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["chosen device cuda\n","Predicted class: 0\n"]}],"source":["# Define a function for prediction\n","def predict_title(model, tokenizer, text):\n","    # Encode the input text\n","    encoded_data = tokenizer(\n","        text,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        padding=True,\n","        truncation=True,\n","        max_length=128,\n","        return_tensors='pt'\n","    )\n","    \n","    # Prepare the input dictionary\n","    inputs = {\n","        'input_ids': encoded_data['input_ids'],\n","        'attention_mask': encoded_data['attention_mask']\n","    }\n","    \n","    # Move input tensors to the appropriate device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(\"chosen device\" , device)\n","    model.model.to(device)\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","    \n","    # Perform a forward pass through the model\n","    with torch.no_grad():\n","        outputs = model.model(**inputs)\n","    \n","    # Get the logits and make predictions\n","    logits = outputs.logits\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","    \n","    return predicted_class\n","\n","# Example usage\n","text_to_predict = \"intro to Gradient Descent\"\n","prediction = predict_title(model, model.tokenizer, text_to_predict)\n","\n","print(f'Predicted class: {prediction}')"]},{"cell_type":"markdown","metadata":{},"source":["calculating accuracy"]},{"cell_type":"markdown","metadata":{},"source":["## Code for Loading a model from a file\n","- run this before the evalutaion and instead of the training method"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def load_checkpoint(checkpoint_path):\n","    # Load the checkpoint\n","    checkpoint = torch.load(checkpoint_path)\n","    \n","    # Retrieve the hyperparameters from the checkpoint\n","    hyperparameters = checkpoint['hyperparameters']\n","    \n","    # Initialize the model using the hyperparameters\n","    model = BertClassifier(\n","        model_name=hyperparameters['model_name'],\n","        num_classes=hyperparameters['num_classes'],\n","        max_length=hyperparameters['max_length'],\n","        batch_size=hyperparameters['batch_size'],\n","        num_epochs=hyperparameters['num_epochs'],\n","        learning_rate=hyperparameters['learning_rate']\n","    )\n","    \n","    # Initialize the optimizer\n","    optimizer = AdamW(model.model.parameters(), lr=hyperparameters['learning_rate'])\n","    \n","    # Load the state dictionaries\n","    model.model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    \n","    # Move the model to the appropriate device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.model.to(device)\n","    \n","    print(f\"Model loaded successfully from {checkpoint_path} with hyperparameters: {hyperparameters}\")\n","    \n","    return model, optimizer, hyperparameters"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_1197826/856266474.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path)\n","/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/roeem/anaconda3/envs/Cuda_DL/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded successfully from ./checkpoint_v3_epoch_9.pt with hyperparameters: {'model_name': 'bert-base-uncased', 'num_classes': 2, 'max_length': 128, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 2e-05}\n"]}],"source":["checkpoint_path = './checkpoint_v3_epoch_9.pt'\n","model, optimizer, hyperparameters = load_checkpoint(checkpoint_path)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNh73rlNhyVA+A6Cn1hl8A3","collapsed_sections":["OO4MHXphmcCv","NzOm33pSmfr3"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
